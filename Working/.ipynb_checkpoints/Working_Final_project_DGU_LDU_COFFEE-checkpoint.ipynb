{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9023628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ce9fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>roaster</th>\n",
       "      <th>roast</th>\n",
       "      <th>loc_country</th>\n",
       "      <th>origin</th>\n",
       "      <th>100g_USD</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ethiopia Shakiso Mormora</td>\n",
       "      <td>Revel Coffee</td>\n",
       "      <td>Medium-Light</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>4.70</td>\n",
       "      <td>92</td>\n",
       "      <td>November 2017</td>\n",
       "      <td>Crisply sweet, cocoa-toned. Lemon blossom, roa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethiopia Suke Quto</td>\n",
       "      <td>Roast House</td>\n",
       "      <td>Medium-Light</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>4.19</td>\n",
       "      <td>92</td>\n",
       "      <td>November 2017</td>\n",
       "      <td>Delicate, sweetly spice-toned. Pink peppercorn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ethiopia Gedeb Halo Beriti</td>\n",
       "      <td>Big Creek Coffee Roasters</td>\n",
       "      <td>Medium</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>4.85</td>\n",
       "      <td>94</td>\n",
       "      <td>November 2017</td>\n",
       "      <td>Deeply sweet, subtly pungent. Honey, pear, tan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethiopia Kayon Mountain</td>\n",
       "      <td>Red Rooster Coffee Roaster</td>\n",
       "      <td>Light</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>5.14</td>\n",
       "      <td>93</td>\n",
       "      <td>November 2017</td>\n",
       "      <td>Delicate, richly and sweetly tart. Dried hibis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ethiopia Gelgelu Natural Organic</td>\n",
       "      <td>Willoughby's Coffee &amp; Tea</td>\n",
       "      <td>Medium-Light</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>3.97</td>\n",
       "      <td>93</td>\n",
       "      <td>November 2017</td>\n",
       "      <td>High-toned, floral. Dried apricot, magnolia, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>Finca Patzibir</td>\n",
       "      <td>El Gran Cafe</td>\n",
       "      <td>Medium-Light</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>4.70</td>\n",
       "      <td>92</td>\n",
       "      <td>November 2022</td>\n",
       "      <td>Crisply sweet, nut-toned. Almond brittle, pie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>Proyecto Aurora</td>\n",
       "      <td>El Gran Cafe</td>\n",
       "      <td>Medium-Light</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>2.94</td>\n",
       "      <td>93</td>\n",
       "      <td>November 2022</td>\n",
       "      <td>Chocolaty, floral-framed. Cocoa nib, honeysuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>Finca El Potrero</td>\n",
       "      <td>El Gran Cafe</td>\n",
       "      <td>Medium-Light</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>2.94</td>\n",
       "      <td>93</td>\n",
       "      <td>November 2022</td>\n",
       "      <td>High-toned, enticingly sweet. Black cherry, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>Chacayá Santiago Atitlán</td>\n",
       "      <td>El Gran Cafe</td>\n",
       "      <td>Medium-Light</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>4.70</td>\n",
       "      <td>93</td>\n",
       "      <td>November 2022</td>\n",
       "      <td>Vibrantly sweet, subtly nuanced. Apricot, dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>Espresso No. 3</td>\n",
       "      <td>El Gran Cafe</td>\n",
       "      <td>Medium-Light</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>5.88</td>\n",
       "      <td>94</td>\n",
       "      <td>November 2022</td>\n",
       "      <td>Evaluated as espresso. Multi-layered, complex....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1246 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name                     roaster  \\\n",
       "0             Ethiopia Shakiso Mormora                Revel Coffee   \n",
       "1                   Ethiopia Suke Quto                 Roast House   \n",
       "2           Ethiopia Gedeb Halo Beriti   Big Creek Coffee Roasters   \n",
       "3              Ethiopia Kayon Mountain  Red Rooster Coffee Roaster   \n",
       "4     Ethiopia Gelgelu Natural Organic   Willoughby's Coffee & Tea   \n",
       "...                                ...                         ...   \n",
       "1241                    Finca Patzibir                El Gran Cafe   \n",
       "1242                   Proyecto Aurora                El Gran Cafe   \n",
       "1243                  Finca El Potrero                El Gran Cafe   \n",
       "1244          Chacayá Santiago Atitlán                El Gran Cafe   \n",
       "1245                    Espresso No. 3                El Gran Cafe   \n",
       "\n",
       "             roast    loc_country     origin  100g_USD  rating    review_date  \\\n",
       "0     Medium-Light  United States   Ethiopia      4.70      92  November 2017   \n",
       "1     Medium-Light  United States   Ethiopia      4.19      92  November 2017   \n",
       "2           Medium  United States   Ethiopia      4.85      94  November 2017   \n",
       "3            Light  United States   Ethiopia      5.14      93  November 2017   \n",
       "4     Medium-Light  United States   Ethiopia      3.97      93  November 2017   \n",
       "...            ...            ...        ...       ...     ...            ...   \n",
       "1241  Medium-Light      Guatemala  Guatemala      4.70      92  November 2022   \n",
       "1242  Medium-Light      Guatemala  Guatemala      2.94      93  November 2022   \n",
       "1243  Medium-Light      Guatemala  Guatemala      2.94      93  November 2022   \n",
       "1244  Medium-Light      Guatemala  Guatemala      4.70      93  November 2022   \n",
       "1245  Medium-Light      Guatemala  Guatemala      5.88      94  November 2022   \n",
       "\n",
       "                                                 review  \n",
       "0     Crisply sweet, cocoa-toned. Lemon blossom, roa...  \n",
       "1     Delicate, sweetly spice-toned. Pink peppercorn...  \n",
       "2     Deeply sweet, subtly pungent. Honey, pear, tan...  \n",
       "3     Delicate, richly and sweetly tart. Dried hibis...  \n",
       "4     High-toned, floral. Dried apricot, magnolia, a...  \n",
       "...                                                 ...  \n",
       "1241  Crisply sweet, nut-toned. Almond brittle, pie ...  \n",
       "1242  Chocolaty, floral-framed. Cocoa nib, honeysuck...  \n",
       "1243  High-toned, enticingly sweet. Black cherry, na...  \n",
       "1244   Vibrantly sweet, subtly nuanced. Apricot, dar...  \n",
       "1245  Evaluated as espresso. Multi-layered, complex....  \n",
       "\n",
       "[1246 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_coffee = pd.read_csv(\"/Users/david/Documents/GitHub/Final_project_DGU_LDU_COFFEE/DATA/simplified_coffee.csv\")\n",
    "S_coffee_df = pd.DataFrame(S_coffee)\n",
    "S_coffee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1de5b4",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a327f0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (63570131.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/d0/kyjzvmwn3fdgc5m0t8snt0nm0000gn/T/ipykernel_23738/63570131.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    from nltk.stem import PorterStemmer\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(stopwords.words('english')) \n",
    "stop_words\n",
    "import string\n",
    "import nltk\n",
    "    from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cc1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316e083",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89141593",
   "metadata": {},
   "source": [
    "3 - Vocabulary Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5425b5a",
   "metadata": {},
   "source": [
    "4 - Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbd642",
   "metadata": {},
   "source": [
    "5 - Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6daa7",
   "metadata": {},
   "source": [
    "Grouping all texts into a dataframe \"corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce759e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = S_coffee_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ba891",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75739f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put all the reviews in a lio\n",
    "corpus=[]\n",
    "for i in S_coffee_df['review']:\n",
    "    corpus.append(nltk.sent_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebce4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a2f62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We flatten the list of lists into only one list\n",
    "corpus = [sentence for sublist in corpus for sentence in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b589d5",
   "metadata": {},
   "source": [
    "## 1 - CLEANING TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce317f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    corpus[i] = re.sub(r'\\W+',' ',corpus[i]) # Replace everything non-alpahnumeric by ' '\n",
    "    corpus[i] = re.sub(r'\\s+',' ',corpus[i]) # Replace one or more whitespaces by  ' '\n",
    "    corpus[i] = re.sub(r'\\d+',' ',corpus[i]) # Replace one or more digits by  ' '\n",
    "    corpus[i] = re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\" \", corpus[i]) # Replace e-mails by ' '\n",
    "    # Replace urls by ''\n",
    "    corpus[i] = re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', ' ' , corpus[i]) \n",
    "    # Replace html tags by ''\n",
    "    corpus[i] = BeautifulSoup(corpus[i], 'html.parser').get_text().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b435851",
   "metadata": {},
   "source": [
    "2 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c11efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    #tokens = nltk.word_tokenize(sentence) # To get the words, it can be also done with sentence.split()\n",
    "    for word in words:\n",
    "        if ( word not in wordfreq.keys() ): ## first time appearnce in the sentence\n",
    "            wordfreq[word] = 1 # We initialize the corresponding counter\n",
    "        else: ## if the world is already existed in the dictionalry \n",
    "            wordfreq[word] += 1 # We increase the corresponding counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87295956",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e1013",
   "metadata": {},
   "source": [
    "## COMMENTS........\n",
    "- sweet and toned appear more than one time in each review on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove the stop words to reducing the corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(stopwords.words('english')) \n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8409570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(stop_words)):\n",
    "    stop_words[i] = re.sub(r\"\\s*'\\s*\\w*\",\"\",stop_words[i])\n",
    "\n",
    "#stop_words = [word for word in list(np.unique(stop_words)) if len(word) > 1]\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524aab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [(wordfreq[key],key) for key in list(wordfreq.keys()) if key not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce738c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042224fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c512de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.sort(reverse = True)\n",
    "\n",
    "# Here we keep only the 100 most frequent words but it can be changed to another bigger value\n",
    "corpus_freq = [(word[1],word[0]) for word in corpus[:301]] \n",
    "corpus_freq = corpus_freq[1:]\n",
    "corpus_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766abf1",
   "metadata": {},
   "source": [
    "**Lemmatization**<br />\n",
    "\n",
    "It will kind of creat cluster of words of the same emotional side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97660469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We would use Lemmatizer to check if we could cluster words that share similar emotions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "corpus_freq = [(lem.lemmatize(word[0]),word[1]) for word in corpus_freq]\n",
    "corpus_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7385a32",
   "metadata": {},
   "source": [
    "**Stemming** <br />\n",
    "\n",
    "to reduce the number of same \"root words\" and be able to look for this extension all over the text and group them into once \"root words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc06fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We manually remove what we sens should not be pertinent enough\n",
    "remove_words = set(['cup', 'coulis', 'satisfying', 'confident', 'shot', 'engaging', 'inviting', 'golden', 'cleanly', 'yet', 'meyer', 'tone', 'though', 'intricately', 'concord', 'bing', 'flavor', 'around', 'gently', 'center', 'short', 'cut', 'pink', 'red', 'small', 'supported', 'evaluated', 'star', 'baker', 'driven', 'part', 'three', 'leaning', 'multi', 'suggestion', 'carry', 'centered', 'nuanced', 'pert', 'note', 'deeply', 'full', 'consolidates',  'finish', 'mouthfeel', 'toned', 'structure', 'notes', 'nib', 'zest', 'long', 'resonant', 'like', 'dried', 'plush', 'baking', 'fresh', 'high', 'hint', 'centers', 'leads', 'hints', 'pipe', 'suggestions', 'lead', 'particular', 'aroma', 'lyrically', 'promise', 'resonantly', '' 'cane', 'using', 'edge', 'edged', 'amplified', 'support', 'throughout', 'richly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_final = [(key, freq) for key,freq in corpus_freq if key not in remove_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0a47d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_final = dict(corpus_final)\n",
    "# my_tuple = corpus_final\n",
    "# my_dict = dict(enumerate(my_tuple))\n",
    "print(corpus_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc07e36",
   "metadata": {},
   "source": [
    "**We assigne each word into their most relevant hypernym groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6139b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# define a dictionary of words\n",
    "words = corpus_final\n",
    "\n",
    "# loop through each word and find its synsets\n",
    "for word in words:\n",
    "    synsets = wordnet.synsets(word)\n",
    "    # loop through each synset and reduce the hierarchy\n",
    "    for synset in synsets:\n",
    "        # get the hypernyms for the synset\n",
    "        hypernyms = synset.hypernyms()\n",
    "        # continue reducing hierarchy until only one hypernym left\n",
    "        while len(hypernyms) > 0:\n",
    "            # get the hypernyms for the first hypernym in the list\n",
    "            new_hypernyms = hypernyms[0].hypernyms()\n",
    "            # if there are no more hypernyms for this hypernym, add its name to the list and break the loop\n",
    "            if len(new_hypernyms) == 0:\n",
    "                words[word].append(hypernyms[0].name())\n",
    "                break\n",
    "            # otherwise, update the list of hypernyms and continue the loop\n",
    "            hypernyms = new_hypernyms\n",
    "\n",
    "# print the reduced synset hierarchy for each word\n",
    "for word in words:\n",
    "    print(word + \": \" + str(words[word]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86a271",
   "metadata": {},
   "source": [
    "Now we keep the most frequent word to start having a final vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22805e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.sort(reverse = True)\n",
    "corpus_freq = [(word[1],word[0]) for word in corpus[:301]] \n",
    "corpus_freq = corpus_freq[1:]\n",
    "corpus_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15efcd5",
   "metadata": {},
   "source": [
    "**To make it even more accurate we gonna remove some irrelevant words manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define the words you want to remove\n",
    "remove_words = set(['cup', 'finish', 'mouthfeel', 'toned', 'structure', 'notes', 'nib', 'zest', 'long', 'resonant', 'like', 'dried', 'plush', 'baking', 'fresh', 'high', 'hint', 'centers', 'leads', 'hints', 'pipe', 'suggestions', 'lead', 'cane', 'using', 'edge', 'edged', 'amplified', 'support', 'throughout'])\n",
    "\n",
    "# Remove the stop words from the text\n",
    "#filtered_words = [word for word in corpus_freq if word.casefold() not in remove_words]\n",
    "#corpus_freq = [(lem.lemmatize(word[0]),word[1]) for word in corpus_freq]\n",
    "corpus = [(wordfreq[key],key) for key in list(wordfreq.keys()) if key not in remove_words]\n",
    "# Join the filtered words back into a sentence\n",
    "#filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f350238",
   "metadata": {},
   "source": [
    "We try to manually put the words into there hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412faf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hypernym groups\n",
    "hypernyms = {\n",
    "    'Fruity': ['fruit', 'lemon', 'apricot', 'cherry', 'tangerine', 'grapefruit', 'orange', 'date', 'plum', 'strawberry', 'mango', 'citrusy', 'peach', 'pomegranate', 'lychee', 'lime', 'tamarind', 'citrus', 'bergamot', 'amber', 'guava', 'grape', 'pear', 'violet', 'pineapple', 'persimmon', 'fruity', 'coconut', 'tropical', 'raisin', 'watermelon', 'plump', 'pie', 'goji', 'fig', 'passionfruit', 'grappa', 'pomelo', 'hibiscus', 'tomato'],\n",
    "    'Earthy': ['mushroom', 'earth'],\n",
    "    'Woody': ['cedar', 'oak', 'fir', 'wood', 'plumeria', 'elm'],\n",
    "    'Berry': ['currant', 'raspberry', 'berry', 'blueberry', 'wine', 'mulberry', 'winy', 'winey', 'sweetness'],\n",
    "    'Sweet': ['tart', 'syrupy', 'sweetly', 'syrup', 'honey', 'caramel', 'fudge', 'nougat', 'molasses', 'sugar', 'toffee', 'candy', 'rum', 'candied', 'jam', 'nutella', 'butterscotch', 'brandy'],\n",
    "    'Nutty': ['almond', 'hazelnut', 'nut', 'cashew', 'pistachio', 'macadamia', 'walnut', 'nutty'],\n",
    "    'Creamy': ['butter', 'milk', 'creamy', 'yogurt'],\n",
    "    'Herbal': ['narcissus', 'freesia', 'honeysuckle', 'maple', 'agave', 'thyme', 'marjoram', 'wisteria', 'musk', 'tobacco', 'ginger', 'herbaceous', 'herb', 'spearmint', 'cardamom', 'lemongrass', 'balm'],\n",
    "    'Chocolaty': ['chocolate', 'cocoa', 'chocolaty', 'cacao'],\n",
    "    'Floral': ['floral', 'flower', 'jasmine', 'magnolia', 'verbena', 'lilac', 'gardenia', 'lavender', 'blossom', 'lily', 'florals', 'rose', 'rhododendron'],\n",
    "    'Texture': ['juicy', 'smooth', 'crisp', 'crisply', 'sandalwood', 'delicate', 'silky', 'viscous', 'drying', 'scorched'],\n",
    "    'Character': ['savory', 'dark', 'satiny', 'balanced', 'rich', 'deep', 'black', 'saturated', 'round', 'aromatic', 'complex', 'pungent', 'rounding', 'fine', 'intensely', 'dry', 'smoky'],\n",
    "    'Acid': ['acidity', 'bittersweet', 'hop', 'zesty'],\n",
    "    'Spice': ['spice', 'peppercorn', 'frankincense', 'spicy', 'cinnamon', 'clove', 'salted', 'tangy'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c09b11",
   "metadata": {},
   "source": [
    "## Creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a19da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {word[0]: [] for word in corpus_freq}\n",
    "reviews = pd.DataFrame(cols)\n",
    "reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
